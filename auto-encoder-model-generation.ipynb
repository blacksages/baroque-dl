{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import everything we will need in our project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "# Useful parameters for reproducibility\n",
    "# SEED = 1234\n",
    "# torch.manual_seed(SEED)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import random, os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import datasets, transforms, utils\n",
    "from os import walk\n",
    "from os.path import join, normpath\n",
    "import pretty_midi\n",
    "\n",
    "from math import ceil\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell is used so that the ploted graph are oppened in a nex window instead of beeing printed in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our <font color=\"red\"> dataloader </font>. This is the object used to load everything we need to train, test and evaluate our model. <font color=\"green\"> This dataloader load and return the whole song.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedSizePianoMusic(data.Dataset):\n",
    "  def __init__(self, size=1000, midi_dir=None, transform=None):\n",
    "    super().__init__()\n",
    "    if midi_dir is None: # Empty dataset\n",
    "      self.musics = []\n",
    "    else: # Non-empty dataset\n",
    "      file_list = list([f\"{root}\\{f}\" for root,d_names,f_names in os.walk(midi_dir) for f in f_names])\n",
    "      # Early loading! splits the musics into blocks of size 'size'\n",
    "      self.musics = []\n",
    "      for f in file_list:\n",
    "        long_music = torch.Tensor(pretty_midi.PrettyMIDI(f).get_piano_roll(fs=100))\n",
    "        long_music = torch.transpose(long_music, 0, 1)\n",
    "        new_size = (len(long_music) // size) * size\n",
    "        long_music = long_music[:new_size] # Cutting off the rest\n",
    "        blocks = torch.reshape(long_music, (len(long_music) // size, size, 128))\n",
    "        for block in blocks:\n",
    "          self.musics.append(block)\n",
    "    self.transform = transform\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.musics)\n",
    "\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "      try:\n",
    "        return self.musics[index] if self.transform is None else self.transform(self.musics[index])\n",
    "      except IndexError:\n",
    "        raise IndexError(\"Item does not exist, have you loaded the MIDI files correctly?\")\n",
    "\n",
    "  def _create_from_self(self, musics):\n",
    "    new_dataset = FixedSizePianoMusic()\n",
    "    new_dataset.musics = musics\n",
    "    new_dataset.transform = self.transform\n",
    "    return new_dataset\n",
    "\n",
    "  def splits(self, test_size=0.3):\n",
    "    train_musics, test_musics = train_test_split(self.musics, test_size=test_size)\n",
    "    return self._create_from_self(train_musics), self._create_from_self(test_musics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the function used to apply all the <font color=\"red\"> transformation </font> we need to apply on the pianorolls before outputing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(element): # Turns all velocity values to 0-1, and extract piano notes\n",
    "  element = torch.Tensor(element)\n",
    "  element = torch.where(element != 0, 1, 0)\n",
    "  element = element[:, 21:109] # 109-21 = 88 => the piano notes of a piano roll\n",
    "  element = element.float()\n",
    "  totals = element.sum(dim=1).reshape((-1,1)) # We want vectors to sum to one\n",
    "  totals[totals == 0] = 1\n",
    "  element /= totals\n",
    "  return torch.transpose(element, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is another <font color=\"red\">transform function</font> that will separate each song in 10 seconds subpassages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_transform(element):\n",
    "    element = torch.Tensor(element)\n",
    "    element = torch.where(element != 0, 1, 0)\n",
    "    element = element[:, 21:109] # 109-21 = 88 => the piano notes of a piano roll\n",
    "    element = element.float()\n",
    "    totals = element.sum(dim=1).reshape((-1,1)) # We want vectors to sum to one\n",
    "    totals[totals == 0] = 1\n",
    "    element /= totals\n",
    "    element = torch.transpose(element, 0, 1)\n",
    "    return element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we will <font color=\"red\"> initiate both the train/test dataset</font> as well as the <font color=\"green\"> train/test loader</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_dataset = FixedSizePianoMusic(midi_dir=\"js/all/\", size=NBR_TIME_STEPS, transform=sub_transform)\\ntest_dataset = FixedSizePianoMusic(midi_dir=\"js/all/\", size=NBR_TIME_STEPS, transform=sub_transform)\\n\\ntrain_loader = data.DataLoader(train_dataset, batch_size=1, shuffle=True)\\ntest_loader = data.DataLoader(test_dataset, batch_size=1, shuffle=True)'"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''train_dataset = FixedSizePianoMusic(midi_dir=\"js/all/\", size=NBR_TIME_STEPS, transform=sub_transform)\n",
    "test_dataset = FixedSizePianoMusic(midi_dir=\"js/all/\", size=NBR_TIME_STEPS, transform=sub_transform)\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=1, shuffle=True)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test what we've got so far (the execution of this cell is not mandatory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'first_train_pianoroll = next(iter(train_loader))\\nfirst_test_pianoroll = next(iter(test_loader))'"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''first_train_pianoroll = next(iter(train_loader))\n",
    "first_test_pianoroll = next(iter(test_loader))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we write a <font color=\"red\">function able to plot the pianorolls</font> that we can use to test different parts of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotPianoRoll(pianoroll, path=None):\n",
    "    fig, ax = plt.subplots(figsize=(160, 60))\n",
    "    ax.imshow(pianoroll, cmap='binary', interpolation='nearest')\n",
    "    if(path != None):\n",
    "        ax.set_title(path)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xlabel('Nbr Timesteps')\n",
    "    ax.set_ylabel('Note value')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test if everything is all right (again, this cell is to be used only for test purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotPianoRoll(first_train_pianoroll[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can get our hands dirty by implementing the <font color=\"red\"> neural network </font> we will use. A possible choice would be a <font color=\"red\">recurent neural network</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAE(nn.Module):\n",
    "    def __init__(self, input_size, latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(88*input_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, latent_dim),\n",
    "\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 88*input_size),\n",
    "            nn.Unflatten(1, (88, input_size)),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "    def decode(self, encoded):\n",
    "        decoded =  self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "class AEModel(nn.Module):\n",
    "    def __init__(self, input_size, latent_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        #these functions are the one used in the encoder\n",
    "        self.maxPool1 = nn.MaxPool1d(kernel_size=2, return_indices=True)\n",
    "        self.maxPool2 = nn.MaxPool1d(kernel_size=2, return_indices=True)\n",
    "        self.maxPool3 = nn.MaxPool1d(kernel_size=2, return_indices=True)\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=88, out_channels=64, kernel_size=3, padding=\"same\"),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, padding=\"same\"),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3, padding=\"same\"),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.reduce = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            #16*125=[2000]\n",
    "            nn.Linear(16*(input_size//8), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, latent_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        #These functions will be used in the decoder\n",
    "        self.maxUnpool1 = nn.MaxUnpool1d(kernel_size=2)\n",
    "        self.maxUnpool2 = nn.MaxUnpool1d(kernel_size=2)\n",
    "        self.maxUnpool3 = nn.MaxUnpool1d(kernel_size=2)\n",
    "        self.convTrans1 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(in_channels=16, out_channels=32, kernel_size=3, padding=1),    \n",
    "        )\n",
    "        self.convTrans2 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(in_channels=32, out_channels=64, kernel_size=3, padding=1),    \n",
    "        )\n",
    "        self.convTrans3 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(in_channels=64, out_channels=88, kernel_size=3, padding=1),    \n",
    "        )\n",
    "        self.expend = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 16*(input_size//8)),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (16, input_size//8))\n",
    "        )\n",
    "        self.out = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #The first step is to encode x\n",
    "        self.indice1 = None\n",
    "        self.indice2 = None\n",
    "        self.indice3 = None\n",
    "        x = self.conv1(x)\n",
    "        x, self.indice1 = self.maxPool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x, self.indice2 = self.maxPool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x, self.indice3 = self.maxPool3(x)\n",
    "        x = self.reduce(x)\n",
    "\n",
    "        #The next step is to decode everything\n",
    "        x = self.expend(x)\n",
    "        x = self.maxUnpool1(x, self.indice3)\n",
    "        x = self.convTrans1(x)\n",
    "        x = self.maxUnpool2(x, self.indice2)\n",
    "        x = self.convTrans2(x)\n",
    "        x = self.maxUnpool3(x, self.indice1)\n",
    "        x = self.convTrans3(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "    def decode(self, latent_space):\n",
    "        x = self.expend(latent_space)\n",
    "        x = self.maxUnpool1(x, self.indice3[0:1])\n",
    "        x = self.convTrans1(x)\n",
    "        x = self.maxUnpool2(x, self.indice2[0:1])\n",
    "        x = self.convTrans2(x)\n",
    "        x = self.maxUnpool3(x, self.indice1[0:1])\n",
    "        x = self.convTrans3(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class BetterAE(nn.Module):\n",
    "    def __init__(self, input_size, lattent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=88, out_channels=128, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=128, out_channels=64, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, lattent_dim),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(lattent_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 64*input_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (64, input_size)),\n",
    "            nn.ConvTranspose1d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(in_channels=128, out_channels=88, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "    def decode(self, encoded):\n",
    "        decoded =  self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, step_size=4):\n",
    "    epoch_loss = 0\n",
    "    loop_count = 0\n",
    "    model.train()\n",
    "\n",
    "    for i, batch in tqdm(enumerate(dataloader), total=len(dataloader), leave=False):\n",
    "        musics = batch.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        # The length of a music is the minimum between its unpadded length and the current loop\n",
    "        predictions = model(musics)\n",
    "        loss = criterion(predictions, musics)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        loop_count += 1\n",
    "\n",
    "    return epoch_loss / loop_count\n",
    "\n",
    "def evaluate(model, dataloader, criterion, step_size=4):\n",
    "    epoch_loss = 0\n",
    "    loop_count = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in tqdm(enumerate(dataloader), total=len(dataloader), leave=False):\n",
    "            musics = batch.to(DEVICE)\n",
    "            # The length of a music is the minimum between its unpadded length and the current loop\n",
    "            predictions = model(musics)\n",
    "            loss = criterion(predictions, musics)\n",
    "            epoch_loss += loss.item()\n",
    "            loop_count += 1\n",
    "\n",
    "    return epoch_loss / loop_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the length of each sub_song. One second = 100 time_steps.\n",
    "# If NBR_TIME_STEPS = 1000, each sub_songs will last 10 seconds\n",
    "NBR_TIME_STEPS = 500\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "NOTE_DIM = 88\n",
    "HIDDEN_DIM = 1024\n",
    "OUTPUT_DIM = 88\n",
    "N_LAYERS = 3\n",
    "DROPOUT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_midi(filename, pianoroll, instrument):\n",
    "    pianoroll = torch.transpose(pianoroll, 0, 1)\n",
    "    pm = pretty_midi.PrettyMIDI()\n",
    "    instrument = pretty_midi.instrument_name_to_program(instrument)\n",
    "    piano = pretty_midi.Instrument(program=instrument)\n",
    "    velocity = 90\n",
    "    current_time = 0\n",
    "    note_list = [0] * 88\n",
    "    for measure in pianoroll:\n",
    "        for current_note in range(len(measure)):\n",
    "            if measure[current_note] != 0 and note_list[current_note] == 0:\n",
    "                note_list[current_note] = current_time\n",
    "            if measure[current_note] == 0 and note_list[current_note] != 0:\n",
    "                note = pretty_midi.Note(\n",
    "                    velocity=velocity,\n",
    "                    pitch=current_note,\n",
    "                    start=note_list[current_note],\n",
    "                    end=current_time\n",
    "                )\n",
    "                note_list[current_note] = 0\n",
    "                piano.notes.append(note)\n",
    "        current_time += 1/100\n",
    "    pm.instruments.append(piano)\n",
    "    pm.write(filename)\n",
    "\n",
    "    return pianoroll.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "  \n",
    "def compute_epochs(model, train_dataloader, test_dataloader, criterion, optimizer, n_epochs, best_path=None, verbose=True):\n",
    "  best_test_loss = float('inf')\n",
    "  criterion = criterion.to(DEVICE)\n",
    "  \n",
    "  train_losses = []\n",
    "  test_losses = []\n",
    "  epoch_times = []\n",
    "\n",
    "  for epoch in range(n_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = train(model, train_dataloader, optimizer, criterion)\n",
    "    test_loss = evaluate(model, test_dataloader, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "\n",
    "    if best_path and test_loss < best_test_loss:\n",
    "      best_test_loss = test_loss\n",
    "      torch.save(model.state_dict(), best_path)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    epoch_times.append(end_time - start_time)\n",
    "\n",
    "    if verbose:\n",
    "      epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "      print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "      print(f'\\tTrain Loss: {train_loss}')\n",
    "      print(f'\\t Test Loss: {test_loss}')\n",
    "    \n",
    "  return model, train_losses, test_losses, epoch_times\n",
    "\n",
    "class ModelProvider():\n",
    "  def __init__(self):\n",
    "    self.AEs= {}\n",
    "  \n",
    "  def convAE(self, index):\n",
    "    return self._fetch_model_(self.AEs, BetterAE, index)\n",
    "  def poolAE(self, index):\n",
    "    return self._fetch_model_(self.AEs, AEModel, index)\n",
    "  def linearAE(self, index):\n",
    "    return self._fetch_model_(self.AEs, SimpleAE, index)\n",
    "  \n",
    "  def _fetch_model_(self, models, ModelClass, index, **kwargs):\n",
    "    if index in models:\n",
    "      return models[index]\n",
    "    else:\n",
    "      models[index] = ModelClass(NBR_TIME_STEPS, HIDDEN_DIM)\n",
    "      models[index].to(DEVICE)\n",
    "      return models[index]\n",
    "\n",
    "def generate_models(train_dataloader, test_dataloader, n_epochs=8):\n",
    "  mp = ModelProvider()\n",
    "  params_list = []\n",
    "  # model_funcs = {'RNN-tanh': mp.RNNtanh, 'RNN-relu': mp.RNNrelu, 'LSTM': mp.LSTM, 'GRU': mp.GRU}\n",
    "  model_funcs = {'CAE': mp.convAE}\n",
    "  # criterions = {'BCE': nn.BCELoss(), 'MSE': nn.MSELoss(), 'CE': nn.CrossEntropyLoss()}\n",
    "  criterions = {'MSE': nn.MSELoss()}\n",
    "  optimizer_funcs = {'ADAM': optim.Adam}\n",
    "  \n",
    "  index = 0\n",
    "  for k0, model_func in model_funcs.items():\n",
    "    for k1, criterion in criterions.items():\n",
    "      for k2, optimizer_func in optimizer_funcs.items():\n",
    "        params_list.append({'name': f'{k0}-{k1}-{k2}', 'model': model_func(index), 'criterion': criterion, 'optimizer': optimizer_func(model_func(index).parameters())})\n",
    "        index += 1\n",
    "\n",
    "  directory = f'models-generation-{n_epochs}epochs-{BATCH_SIZE}batchs-{time.time()}'\n",
    "  os.makedirs(directory)\n",
    "\n",
    "  for i, params_row in enumerate(params_list):\n",
    "    print(f'Processing model {i+1}/{len(params_list)} {params_row[\"name\"]}')\n",
    "    os.makedirs(f'{directory}/{params_row[\"name\"]}')\n",
    "    model, train_losses, test_losses, epoch_times = compute_epochs(params_row['model'], train_dataloader, test_dataloader, params_row['criterion'], params_row['optimizer'], n_epochs, \n",
    "                                                                   best_path=f'{directory}/{params_row[\"name\"]}/{params_row[\"name\"]}.pt', \n",
    "                                                                   verbose=False)\n",
    "    \n",
    "    # Saving losses and times to CSV\n",
    "    pd.DataFrame.from_dict({'train_loss':train_losses, 'test_loss': test_losses, 'epoch_time': epoch_times}).to_csv(f'{directory}/{params_row[\"name\"]}/{params_row[\"name\"]}.csv')\n",
    "    # Reloading the model with its best one (best test loss)\n",
    "    model = params_row['model']\n",
    "    model.load_state_dict(torch.load(f'{directory}/{params_row[\"name\"]}/{params_row[\"name\"]}.pt'))\n",
    "    # Sampling a music for generation\n",
    "    dataset = FixedSizePianoMusic(midi_dir=\"./js/bigMix\", size=NBR_TIME_STEPS, transform=sub_transform)\n",
    "    random_sample = dataset[random.randint(0, len(dataset))]\n",
    "    create_midi(f'original-sample.mid', random_sample, 'Acoustic Grand Piano')\n",
    "    random_sample = random_sample.cuda()\n",
    "    print(random_sample.get_device())\n",
    "    #print(model.get_device())\n",
    "    random_sample = torch.reshape(random_sample, (1, 88, 500))\n",
    "    # Generating a music based on a few notes of the sample\n",
    "    print(random_sample.shape)\n",
    "    roll = model.forward(random_sample)\n",
    "    print(roll[0])\n",
    "    roll = torch.where(roll >= 1/10, 1, 0)\n",
    "    create_midi(f'uncompressed-sample.mid', roll[0], 'Acoustic Grand Piano')\n",
    "    latent_space = torch.randn(1,HIDDEN_DIM)\n",
    "    model = model.to(DEVICE)\n",
    "    latent_space = latent_space.to(DEVICE)\n",
    "    #latent_space = latent_space.to(DEVICE)\n",
    "    result = model.decode(latent_space)\n",
    "    # Cleaning the result\n",
    "    pianoroll = torch.where(result >= 1/10, 1, 0)\n",
    "    model = model.cpu()\n",
    "    pianoroll = pianoroll.to('cpu')\n",
    "    x_np = pianoroll.detach().numpy()[0]\n",
    "    print(x_np.shape)\n",
    "    x_df = pd.DataFrame(x_np)\n",
    "    # Saving the generation as a CSV\n",
    "    x_df.to_csv(f'{directory}/{params_row[\"name\"]}/{params_row[\"name\"]}-sample.csv')\n",
    "    # Saving the generation as a MIDI\n",
    "    print(pianoroll[0][5])\n",
    "    print(pianoroll.shape)\n",
    "    create_midi(f'{directory}/{params_row[\"name\"]}/{params_row[\"name\"]}-sample.mid', pianoroll[0], 'Acoustic Grand Piano')\n",
    "    #plotPianoRoll(pianoroll[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FixedSizePianoMusic(midi_dir=\"./js/bigMix\", size=NBR_TIME_STEPS, transform=sub_transform)\n",
    "\n",
    "train_dataset, test_dataset = dataset.splits()\n",
    "\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing model 1/1 CAE-MSE-ADAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([1, 88, 500])\n",
      "tensor([[0.0074, 0.0058, 0.0059,  ..., 0.0060, 0.0056, 0.0069],\n",
      "        [0.0074, 0.0058, 0.0059,  ..., 0.0060, 0.0056, 0.0069],\n",
      "        [0.0074, 0.0058, 0.0059,  ..., 0.0060, 0.0056, 0.0069],\n",
      "        ...,\n",
      "        [0.0074, 0.0058, 0.0059,  ..., 0.0060, 0.0056, 0.0069],\n",
      "        [0.0074, 0.0058, 0.0059,  ..., 0.0060, 0.0056, 0.0069],\n",
      "        [0.0074, 0.0058, 0.0059,  ..., 0.0060, 0.0056, 0.0069]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "(88, 500)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "torch.Size([1, 88, 500])\n"
     ]
    }
   ],
   "source": [
    "generate_models(train_loader, test_loader, n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
